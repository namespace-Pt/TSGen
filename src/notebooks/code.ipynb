{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-24 06:26:26,143] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-24 06:26:28,876] INFO (Config) setting seed to 42...\n",
      "[2024-01-24 06:26:28,882] INFO (Config) setting PLM to t5...\n",
      "[2024-01-24 06:26:29,085] INFO (Config) Config: {'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'batch_size': 2, 'bf16': False, 'cache_root': 'data/cache/MSMARCO-doc', 'data_format': 'memmap', 'data_root': '/share/peitian/Data/Datasets/Adon', 'dataset': 'MSMARCO-doc', 'debug': False, 'deepspeed': None, 'device': 0, 'distill_src': 'none', 'early_stop_patience': 5, 'enable_all_gather': True, 'enable_distill': False, 'enable_inbatch_negative': True, 'epoch': 20, 'eval_batch_size': 2, 'eval_delay': 0, 'eval_flops': False, 'eval_metric': ['mrr', 'recall'], 'eval_metric_cutoff': [1, 5, 10, 100, 1000], 'eval_mode': 'retrieve', 'eval_posting_length': False, 'eval_set': 'dev', 'eval_step': '1e', 'fp16': False, 'grad_accum_step': 1, 'hits': 1000, 'index_shard': 32, 'index_thread': 10, 'index_type': 'invvec', 'learning_rate': 3e-06, 'load_ckpt': None, 'load_encode': False, 'load_index': True, 'load_query_encode': False, 'load_result': False, 'load_text_encode': False, 'loader_train': 'neg', 'main_metric': 'Recall@10', 'max_grad_norm': 0, 'max_query_length': 64, 'max_step': 0, 'max_text_length': 512, 'mode': 'train', 'model_type': None, 'neg_type': 'random', 'nneg': 1, 'num_worker': 0, 'parallel': 'text', 'plm': 't5', 'plm_dir': '/share/peitian/Data/Datasets/Adon/PLMs/t5', 'plm_root': '/share/peitian/Data/Datasets/Adon/PLMs', 'plm_tokenizer': 't5', 'posting_prune': 0.0, 'query_gate_k': 0, 'query_length': 64, 'report_to': 'none', 'return_first_mask': False, 'return_special_mask': False, 'save_at_eval': False, 'save_ckpt': 'best', 'save_encode': False, 'save_index': True, 'save_model': False, 'save_res': 'retrieval_result', 'save_score': False, 'scheduler': 'constant', 'seed': 42, 'special_token_ids': {'cls': (None, None), 'pad': ('<pad>', 0), 'unk': ('<unk>', 2), 'sep': (None, None), 'eos': ('</s>', 1)}, 'text_col': [2, 1, 3], 'text_col_sep': ' ', 'text_gate_k': 0, 'text_length': 512, 'text_type': 'default', 'train_set': ['train'], 'untie_encoder': False, 'verifier_hits': 1000, 'verifier_index': 'none', 'verifier_src': 'none', 'verifier_type': 'none', 'vocab_size': 32100, 'warmup_ratio': 0.1, 'warmup_step': 0, 'weight_decay': 0.01}\n",
      "[2024-01-24 06:26:29,132] INFO (Dataset) initializing MSMARCO-doc memmap Text dataset...\n",
      "[2024-01-24 06:26:29,184] INFO (Dataset) initializing MSMARCO-doc memmap Query dev dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "if sys.path[-1] != \"../\":\n",
    "    sys.path.append(\"../\")\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from IPython.display import display\n",
    "from random import sample\n",
    "from transformers import AutoModel, AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models.AutoModel import AutoModel as AM\n",
    "from utils.util import *\n",
    "from utils.index import *\n",
    "from utils.data import *\n",
    "\n",
    "from hydra import initialize, compose\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        # \"base=NQ320k\",\n",
    "        \"base=MSMARCO-doc\",\n",
    "        \"++text_col=[2,1,3]\",\n",
    "        # \"base=MS300k\",\n",
    "        # \"++plm=t5\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "\n",
    "loaders = prepare_data(config)\n",
    "\n",
    "loader_text = loaders[\"text\"]\n",
    "loader_query = loaders[\"query\"]\n",
    "text_dataset = loader_text.dataset\n",
    "query_dataset = loader_query.dataset\n",
    "\n",
    "# train_dataset = prepare_train_data(config, loader_text.dataset)\n",
    "# train_query_dataset = train_dataset.query_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_type = \"term\"\n",
    "# code_type = \"2gram\"\n",
    "# code_type = \"id\"\n",
    "code_tokenizer = \"t5\"\n",
    "# code_length = 26\n",
    "code_length = 32\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "# trie = TrieIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\")\n",
    "# trie.load()\n",
    "\n",
    "# wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "# wordset.fit(None)\n",
    "\n",
    "# df = pd.DataFrame(text_codes)\n",
    "# duplicates = df.groupby(df.columns.tolist(),as_index=False).size()\n",
    "# duplicates = duplicates.sort_values(\"size\", ascending=False)\n",
    "# duplicates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# dup = df.duplicated(keep=False).to_numpy()\n",
    "# dup_indices = np.argwhere(dup)[:, 0]\n",
    "# len(dup_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> kidney, foods, eat, creatinine, creatine, creatinine2013, blood, high, level, snack, mean,</s><pad>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['http://www.kidney-treatment.org/creatinine/162.html What Foods Not to Eat When Having High Creatinine Font Size A A AWhat Foods Not to Eat When Having High Creatinine2013-10-04 10:31What foods not to eat when having high creatinine? To know well about the answer, we need to firstly understand what is creatinine and what high creatinine level mean. What is creatinine? Creatinine is a breakdown product of creatine, which is an important part of muscle. In normal condition, creatinine is filtered by glomerulus and then discharged with urine. Therefore, creatinine level in blood is always kept in a stable range 0.5-1.3mg/d L. What does high creatinine level mean? Since kidneys are responsible for discharging creatinine in blood, so when kidneys are injured for some reason, creatinine level in blood increases. Serum creatinine level reflects how well kidney functions, so patients are suggested to test their creatinine level in blood, so as to judge whether kidney function is affected. Creatinine is one of the wastes produced in the body. When kidneys are damaged, not only creatinine, but also other wastes can not be excreted. Therefore, high creatinine level indicates there are lots of wastes in blood. What foods not to eat when having high creatinine? High creatinine level indicates kidney function has been affected. Under such a condition, patients need to be very careful about foods they eat in their daily life, so as to avoid increasing burden on kidney or causing further kidney damages. Basing on this point, patients generally need to:1. Adjust protein intake For patients living with high creatinine level caused by kidney disease, they need to adjust their proper intake. If they have not start dialysis, they generally need to limit protein intake and this need them to stay far away from high protein foods like bean and bean products. However, if they have started dialysis, much more protein will be needed everyday.2. Avoid salted foods Salt is rich in sodium. Eating too much high sodium foods like pickle, soy sauce, cheese, bacon and snack foods will increase blood pressure and trigger fluid retention.3. High potassium foods With high creatinine level</s>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# indices = random.sample(range(len(text_dataset)), 5)\n",
    "# indices = range(10)\n",
    "indices = [3088934]\n",
    "# indices = dup_indices\n",
    "text_code = text_codes[indices]\n",
    "text_code[text_code == -1] = 0\n",
    "# display(text_code)\n",
    "display(t.batch_decode(text_code))\n",
    "display(t.batch_decode(np.array(text_dataset[indices][\"text\"][\"input_ids\"])[:, :1500]))\n",
    "\n",
    "# most_dup_idx = np.argwhere((text_codes == duplicates.iloc[0].to_numpy()[:-1]).all(-1))[:, 0]\n",
    "# most_dup_code = text_codes[most_dup_idx]\n",
    "# most_dup_code[most_dup_code == -1] = 0\n",
    "# most_dup_text = np.array(text_dataset[most_dup_idx][\"text\"][\"input_ids\"])[:, :code_length + 5]\n",
    "# t.batch_decode(most_dup_code), t.batch_decode(most_dup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19049, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get document container specific words\n",
    "wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "wordset.fit(None)\n",
    "word = t.encode(\"sodium\")[:-1]\n",
    "wordset.vocab[word + [6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3213835/3213835 [01:19<00:00, 40388.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert to LLM-Indexer\n",
    "\n",
    "with open(\"/share/peitian/Data/Datasets/model-based-ir/msmarco-doc/docid.unicoil-term-comma-32.json\", \"w\") as f:\n",
    "    for i, code in enumerate(tqdm(text_codes)):\n",
    "        code = code[1:]\n",
    "        code = code[code != -1]\n",
    "        code = t.decode(code, skip_special_tokens=True).strip(\",\")\n",
    "        if len(code) == 0:\n",
    "            print(i)\n",
    "            break\n",
    "            code = \"Empty document\"\n",
    "        f.write(json.dumps({\"docid\": code}, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-20 04:02:18,510] INFO (Dataset) initializing MSMARCO-doc memmap Query dev dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Query                                             : does xpress bet charge to deposit money in your account\n",
      "Text                                              : http://www.xpressbet.com/paynearme. We make wagering easy for you Online Log In to our safe and secure wagering environment for a fast and efficient wagering experience. Mobile Mobile wagering is available on most smartphones and tablets, making it easy to wager anytime, anywhere. Phone Just pick up the phone to speak with one of our Live Tellers or use our Automated Voice & Touch Tone services. Curious? See our interface\n",
      "Code                                              : paynearme, wager, xpressbet, tellers, mobile, anytime, smartphones, easy, anywhere, tablets, secure,\n",
      "**************************************************\n",
      "Query                                             : how much is a cost to run disneyland\n",
      "Text                                              : https://www.quora.com/What-is-the-operating-cost-of-Disneyland-per-day What is the operating cost of Disneyland per day? \"Quora User Answered Aug 4, 2016  Author has 685 answers and 499.9k answer views. There are some really spurious answers given here. INSTEAD: Here are the financial reports filed by Disney Corp. In fiscal 2015, Disney earned US\n",
      "Code                                              : disneyland, disney, day, cost, parks, operating, per, much, money, revenue, resorts, fiscal,\n",
      "**************************************************\n",
      "Query                                             : botulinum definition\n",
      "Text                                              : https://www.merriam-webster.com/dictionary/botulinum%20toxin botulinum toxin \"botulinum toxinnoun Updated on: 4 Apr 2018Trending Now:oligarch glib depose omnibus acrimony ALLTime Traveler! Explore the year a word first appeared Definition of botulinum toxin: a neurotoxin formed\n",
      "Code                                              : toxin, toxin1923medical, toxinnoun, 20toxin, toxinmedical,\n",
      "**************************************************\n",
      "Query                                             : do physicians pay for insurance from their salaries?\n",
      "Text                                              : https://medicineandthemilitary.com/careers-and-benefits/physician-salary-and-benefits Physician Salary + Benefits Physician Salary + Benefits The Military provides benefits for physicians and their families competitive with what is offered in the civilian world. These benefits include not only pay, but also repayment of student loans and various types of insurance coverage. Base Pay + Allowances All active-duty physicians receive base pay,\n",
      "Code                                              : physician, military, salary, benefits, pay, paid, army, active, guard, duty, reserve, medical, service, uniformed,\n",
      "**************************************************\n",
      "Query                                             : here there be dragons comic\n",
      "Text                                              : https://en.wikipedia.org/wiki/James_A._Owen James A. Owen \"From Wikipedia, the free encyclopedianavigation search [ hide]This article has multiple issues. Please help improve it or discuss these issues on the talk page. ( Learn how and when to remove these template messages)This biography of a living person needs additional citations for verification. (March 2012)This biographical article relies too much on references\n",
      "Code                                              : owen, james, starchild, jamesaowen, illustrator, chronicles, starchild2, hollow3,\n",
      "**************************************************\n",
      "Query                                             : blood diseases that are sexually transmitted\n",
      "Text                                              : http://www2.gnb.ca/content/gnb/en/departments/ocmoh/cdc/content/sexually_transmitteddiseasesandinfections.html Sexually transmitted and blood borne infections (STBBI) Sexually transmitted and blood borne infections (STBBI)A sexually transmitted infection (STI) is an infection that can be transmitted from one person to another through sexual contact (ex\n",
      "Code                                              : stbbi, sexually, blood, transmitted, infection, sti, transmission, bbi, borne,\n",
      "**************************************************\n",
      "Query                                             : define bona fides\n",
      "Text                                              : https://en.wikipedia.org/wiki/Bona_fide Good faith \"From Wikipedia, the free encyclopedia (Redirected from Bona fide)navigation search\"\"Assume good faith\"\" redirects here. For the Wikipedia editing guideline, see Wikipedia: Assume good faith. \"\"Bona fide\"\" redirects here. For other uses, see Bona fide (disambiguation)\n",
      "Code                                              : fide, faith, fides2, bona, good, sincere, assume, latin, law, reliability, wikipedia,\n",
      "**************************************************\n",
      "Query                                             : effects of detox juice cleanse\n",
      "Text                                              : http://www.livestrong.com/article/112648-side-effects-detox-cleanse/ The Side Effects of a Detox Cleanse The Side Effects of a Detox Cleanseby JILL CORLEONE, RDN, LD Oct. 03, 2017If your diet has comprised fast food and alcohol for far too long, you may be tempted to try a detox cleanse to help rid\n",
      "Code                                              : cleanses, detox, cleanseby, side, following, diet, metabolism, rids, effects, cayenne, dietitian,\n",
      "**************************************************\n",
      "Query                                             : do prince harry and william have last names\n",
      "Text                                              : https://answers.yahoo.com/question/index?qid=20070625115037AAXVASd Do Prince William and Prince Harry have last names? \"Society & Culture Royalty Do Prince William and Prince Harry have last names? I always wondered. The only thing I know is that William is the Prince of whales. I never hear anyone talk about their last names.2 following 16 answers Answers Relevance Rating Newest\n",
      "Code                                              : harry, william, prince, name, surname, last, highness, royal, princess, whales, windsor,\n",
      "**************************************************\n",
      "Query                                             : can hives be a sign of pregnancy\n",
      "Text                                              : http://americanpregnancy.org/womens-health/hives-during-pregnancy/ Hives During Pregnancy Hives During Pregnancy Home / Women's Health / Hives During Pregnancy About 1 in 5 pregnant women experience changes in their skin during pregnancy, including acne, skin darkening, and stretch marks. Although women may feel self-conscious about these new “beauty marks,”\n",
      "Code                                              : hives, pregnancy, pregnant, during, pupp, women, skin, allergic, itching, changes, stretch, experience, while,\n",
      "**************************************************\n",
      "Query                                             : causes of petechial hemorrhage\n",
      "Text                                              : http://www.home-remedies-for-you.com/articles/2519/diseases-and-ailments/petechial-hemorrhage.html Petechial Hemorrhage Home Articles Diseases and Ailments Petechial Hemorrhageby Sharon Hopkins Tweet Petechial hemorrhage is also known as capillary hemorrhage, as it occurs when ca\n",
      "Code                                              : petechial, petechiae, hemorrhage, brain, hemorrhageby, blood, escape,\n",
      "**************************************************\n",
      "Query                                             : how long does it take to get your bsrn if you already have a bachelors degree\n",
      "Text                                              : http://www.excite.com/education/nursing/rn-to-bsn RN to BSN Degrees RN to BSN Degrees BSN stands for Bachelor of Science in nursing whereas RN stands for registered nurse. There are currently two types of registered nursing degrees being offered by colleges and universities in the US. You can gain an associate's degree in nursing or a bachelors of science in nursing. An Associate’s degree takes\n",
      "Code                                              : nursing, rn, bsn, degree, bachelor, associate, stands, registered, science, completion, qualified, years,\n",
      "**************************************************\n",
      "Query                                             : symptoms of ptsd in vietnam veterans\n",
      "Text                                              : https://consumer.healthday.com/general-health-information-16/military-health-news-763/ptsd-symptoms-persist-for-thousands-of-vietnam-vets-study-finds-701554.html PTSD Symptoms Persist for Thousands of Vietnam Vets, Study Finds \"PTSD Symptoms Persist for Thousands of Vietnam Vets\n",
      "Code                                              : vietnam, ptsd, war, veterans, persist, vets, traumatic, military, symptoms, stress, vulnerable,\n"
     ]
    }
   ],
   "source": [
    "# check text_codes and text\n",
    "positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "query_dataset = QueryDataset(config, \"dev\")\n",
    "for k, v in positives.items():\n",
    "    pos_index = v[0]\n",
    "    query = query_dataset[k][\"query\"][\"input_ids\"]\n",
    "    text = text_dataset[pos_index][\"text\"][\"input_ids\"]\n",
    "    text_code = text_codes[pos_index]\n",
    "    \n",
    "    query = t.decode(query, skip_special_tokens=True)\n",
    "    text = t.decode(text[:100], skip_special_tokens=True)\n",
    "    text_code = t.decode(text_code[text_code != -1], skip_special_tokens=True)\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    print(f\"{'Query': <50}: {query}\")\n",
    "    print(f\"{'Text': <50}: {text}\")\n",
    "    print(f\"{'Code': <50}: {text_code}\")\n",
    "\n",
    "    x = input()\n",
    "    if x == \"s\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['http://chemdemos.uoregon.edu/demos/Precipitation-of-Silver-Chloride Please read the following disclaimer carefully \"Precipitation of Silver Chloride When clear, colorless silver nitrate solution is added to clear, colorless sodium chloride solution, white silver chloride precipitates. Curriculum Notes This demo is typically performed when solubility rules are being presented. Allow two minutes for',\n",
       "  'http://www.myfitnesspal.com/food/calories/homemade-tomato-sandwich-2-slices-white-bread-1-fresh-tomato-12945968 Calories in Homemade Tomato Sandwich 2 Slices White Bread, 1 Fresh Tomato, Calories in Homemade Tomato Sandwich 2 Slices White Bread, 1 Fresh Tomato,Nutrition Facts Homemade Tomato Sandwich - 2 Slice',\n",
       "  'https://www.quora.com/Why-is-a-yellow-color-more-common-in-city-street-lighting-than-white Why is a yellow color more common in city street lighting than white? Brandon Row, House design lover Photograph lover Answered Aug 20, 2017Most street lighting comes in the form of high pressure sodium lamps, which produce the most amount of light for the least amount of electricity required to power them.',\n",
       "  'https://www.reference.com/health/many-calories-egg-white-6093cd2bdb85fd8c How Many Calories Are in an Egg White? Health Nutrition & Diets Calories in Food Q: How Many Calories Are in an Egg White? A: Quick Answer There are 16 calories in an egg white, and one calorie is from fat. One egg white contains 2 percent sodium, 2',\n",
       "  'http://www.myfitnesspal.com/food/calories/white-onion-medium-size-370849054?v2=false Calories in White Onion Medium Size Calories in White Onion Medium Size Nutrition Facts White Onion - Medium Size Servings: Calories 170 Sodium 65 mg Total Fat 0 g Potassium 0 mg Saturated 0 g',\n",
       "  'http://www.myfitnesspal.com/food/calories/egg-white-extra-large-egg-59588007 Calories in Egg White - Extra Large Egg Calories in Egg White - Extra Large Egg Nutrition Facts Egg White - - Extra Large Egg Servings: Calories 660 Sodium 0 mg Total Fat 22 g Potassium 0 mg Saturated 0 g Total Carb',\n",
       "  'http://www.livestrong.com/article/442803-is-yellow-rice-healthy-for-you/ Is Yellow Rice Healthy for You? Is Yellow Rice Healthy for You?by JILL CORLEONE, RDN, LD Oct. 03, 2017In many Hispanic homes, yellow rice is a meal staple. While recipes vary, yellow rice is usually made with white rice and onions, with spices such as s',\n",
       "  'http://www.myfitnesspal.com/food/calories/cooked-white-mushrooms-cooked-380635807?v2=false Calories in Cooked White Mushrooms Cooked Calories in Cooked White Mushrooms Cooked Nutrition Facts Cooked White Mushrooms - Cooked Servings: Calories 500 Sodium 0 mg Total Fat 0',\n",
       "  'https://answers.yahoo.com/question/index?qid=20120303001702AAh2TkP Why does sodium chloride give a white precipitate with silver nitrate solution but CCl4 does not? Science & Mathematics Chemistry Why does sodium chloride give a white precipitate with silver nitrate solution but CCl4 does not? Follow 2 answers Answers Relevance Rating Newest Oldest',\n",
       "  'http://www.myfitnesspal.com/food/calories/home-golumpki-with-white-rice-ground-beef-45441473 Calories in Home Golumpki With White Rice & Ground Beef Calories in Home Golumpki With White Rice & Ground Beef Nutrition Facts Home - Golumpki With White Rice & Ground Beef Servings: Calories 380 ',\n",
       "  'https://www.fatsecret.com/calories-nutrition/food/white-bread/sodium Sodium in White Bread \"Food database and calorie counter Sodium in White Bread The favorite choice for the term \"\"White Bread\"\" is 1 slice of White Bread which has about 170 mg of sodium. The amount of salt (sodium) for a variety of types and serving sizes of White Bread is shown below. View other nutritional values (such',\n",
       "  'http://www.myfitnesspal.com/food/calories/sheetz-blt-sandwich-on-white-with-mayo-93815891 Calories in Sheetz Blt (Sandwich on White, With Mayo) Calories in Sheetz Blt (Sandwich on White, With Mayo)Nutrition Facts Sheetz - Blt (Sandwich on',\n",
       "  'http://snowwhite.co.in/ Home Profile Contact Us About Us Online EnquiryManufactures Of Chemicals CHEMICALSMono/Di/Tri Sodium Phosphate Mono Sodium phosphate is a generic term for the salts of sodium and phosphate. Read More... Sodium Di-Acetate Sodium DI Acetate (diacetato de sodio, natriumdi',\n",
       "  'https://en.wikipedia.org/wiki/White_liquor White liquor White liquor is a strongly alkaline solution mainly of sodium hydroxide and sodium sulfide. It is used in the first stage of the Kraft process in which lignin and hemicellulose are separated from cellulose fiber for the production of pulp. [1] The white liquor breaks the bonds between lignin and cellulose. It is called white liquor',\n",
       "  'http://www.myfitnesspal.com/food/calories/birthday-cake-with-buttercream-frosting-small-slice-white-cake-234457478 Calories in Birthday Cake With Buttercream Frosting Small Slice White Cake Calories in Birthday Cake With Buttercream Frosting Small Slice White Cake Nutrition Facts Birthday Cake With Buttercream Frosting - Small Slice - White Cake Servings: Calories 118',\n",
       "  'http://www.myfitnesspal.com/food/calories/egg-white-omelette-3-egg-white-veggie-omelette-60913199 Calories in Egg White Omelette 3 Egg White Veggie Omelette Calories in Egg White Omelette 3 Egg White Veggie Omelette Nutrition Facts Egg White Omelette - 3 Egg White Veggie Omelette Servings: Calo',\n",
       "  'http://www.bonappetit.com/test-kitchen/cooking-tips/article/baking-soda Baking Soda Baking soda (sodium bicarbonate), a fine white powder, is a quick-acting leavener used in baking. When baking soda (which is alkaline) is blended with moisture and an acidic ingredient (such as yogurt, buttermilk, molasses, brown',\n",
       "  'http://www.myfitnesspal.com/food/calories/egg-white-only-1-hard-boiled-140421992 Calories in Egg White Only 1 Hard Boiled Calories in Egg White Only 1 Hard Boiled Nutrition Facts Egg White Only - 1 Hard Boiled Servings: Calories 207 Sodium 266 mg Total Fat 4 g Potassium 394 mg Saturated 1 g Total Carbs',\n",
       "  'https://www.reference.com/science/sodium-conductor-electricity-5ce540a78490439f Is sodium a conductor of electricity? \"Science Chemistry Q: Is sodium a conductor of electricity? A: Quick Answer Sodium is a soft, silver-white metallic element known for its electrical and thermal conductivity. At room temperature sodium is waxlike in composition and is malleable and ductile. ',\n",
       "  'http://www.fooducate.com/product/McDonald%20s%20Egg%20White%20Delight/AEE08D54-C062-11E2-9B11-1231381A4CEA McDonald\\'s Egg White Delight \"Product Fast Food Breakfast Mc Donald\\'s Egg White Delightnutrition grade C plus 250Calories Per Serving34 comments Rating: 4.31 with 197 ratings 15 followers Ex',\n",
       "  'http://www.myfitnesspal.com/food/calories/grilled-cheese-sandwich-2-slices-of-cheese-white-bread-2199310 Calories in Grilled cheese sandwich 2 slices of cheese, white bread Calories in Grilled cheese sandwich 2 slices of cheese, white bread Nutrition Facts Grilled cheese sandwich - 2 slices of cheese, white bread Servings: Cal',\n",
       "  'http://www.myfitnesspal.com/food/calories/lindt-lindor-truffles-white-chocolate-with-a-smooth-filling-70128536 Calories in Lindt Lindor Truffles, White Chocolate With A Smooth Filling Calories in Lindt Lindor Truffles, White Chocolate With A Smooth Filling Nutrition Facts Lindt - Lindor Truffles, White Chocolate',\n",
       "  'http://www.myfitnesspal.com/food/calories/white-peach-fresh-medium-2-2-3-net-carbs-94376517 \"Calories in White Peach Fresh Medium 2 -2/3\"\" (Net Carbs)\" \"Calories in White Peach Fresh Medium 2 -2/3\"\" (Net Carbs)Nutrition Facts White Peach - Fresh Medium 2 -2/3',\n",
       "  'http://www.myfitnesspal.com/food/calories/chilis-potato-skins-appetizer-157914255 Calories in Chilis Potato Skins Appetizer Calories in Chilis Potato Skins Appetizer Nutrition Facts Chilis Potato Skins - Appetizer Servings: Calories 390 Sodium 54 mg Total Fat 7 g Potassium 11 mg Saturated',\n",
       "  'https://www.futurederm.com/is-it-dangerous-to-use-household-bleach-to-whiten-your-teeth/ Is it Dangerous to Use Household Bleach to Whiten Your Teeth? October 30, 2013Is it Dangerous to Use Household Bleach to Whiten Your Teeth? Skin Careby Natalie Bell. I’ve seen some strange home remedies in my day',\n",
       "  'http://www.myfitnesspal.com/food/calories/generic-grilled-ham-and-cheese-white-bread-with-spread-8241956 Calories in Grilled Ham and Cheese (White Bread) With Spread Calories in Grilled Ham and Cheese (White Bread) With Spread Nutrition Facts Generic - Grilled Ham and Cheese (White Bread) With Spread Servings: Calo',\n",
       "  'http://www.myfitnesspal.com/food/calories/lol-white-american-cheese-79792490 Calories in lol white american cheese Calories in lol white american cheese Nutrition Factslol white american - cheese Servings: Calories 80 Sodium 0 mg Total Fat 0 g Potassium 0 mg Saturated 0 g Total Carbs 0 g'],\n",
       " ['silver, chloride, nitrate, precipitation, demo, cl, clear, sodium, white, solution, solutionglass,',\n",
       "  'sandwich, tomato, bread, homemade, slices, 2, white, calories, fresh, 163, hummus, sodium, carbs,',\n",
       "  'street, lighting, yellow, white, lamps, color, city, orange, common, sodium, lit, more, why, darker, glow,',\n",
       "  'egg, white, calories, caloriecount, protein, how, many, sodium, scrambled, potassium, whole, cholesterol,',\n",
       "  'onion, white, medium, calories, false, size, cheddar, 170, 46, red, sodium, values, 65, mg,',\n",
       "  'egg, white, extra, calorie, large, 660, 109, food, sodium, carbs, patty, 2000,',\n",
       "  'rice, yellow, white, carbs, healthy, cooked, carbohydrates, calories, sodium, onions, folate, cup, 45, iron,',\n",
       "  'mushrooms, mushrooms1, white, cooked, calories, false, raw, pasta, sodium, quinoa, rice, food, 500,',\n",
       "  'chloride, sodium, silver, white, dissolve, cl, nitrate, ccl4, salt, water,',\n",
       "  'golumpki, rice, white, ground, beef, calories, home, 380, 115, mg, carbs, sodium,',\n",
       "  'sodium, bread, white, mg, salt, breadview, sizes, slice, 170, calories, carbs, choice, favorite,',\n",
       "  'sheetz, blt, sandwich, white, mayo, calories, carbs, 133, sodium, 279, mg,',\n",
       "  'snowwhite, snow, sodium, white, chemicals, phosphate, tri, products, intermediates, di, manufacturer, companies, mono,',\n",
       "  'liquor, white, kraft, pulp, alkali, sodium, cellulose, carbonate, sulfide, mainly,',\n",
       "  'birthday, cake, buttercream, frosting, white, small, calorie, slice, 233, mg, carbs, values, sodium,',\n",
       "  'white, egg, omelette, calorie, veggie, 3, carbs, 326, 109, food, values, sodium,',\n",
       "  'baking, soda, powder, substitute, bicarbonate, leavener, tartar, teaspoon, sodium, rise, acidic, white,',\n",
       "  'white, egg, boiled, calorie, only, hard, xl, 207, only1, sodium, carbs,',\n",
       "  'sodium, conductor, electricity, na, element, chemistry, conductivity, thermal, metallic, silver, white, science,',\n",
       "  'delight, delightnutrition, mcdonald, white, datem, fooducate, datemdatem, sodium,',\n",
       "  'sandwich, grilled, cheese, bread, 2, slices, white, calories, mg, carbs, sodium, 0, 30,',\n",
       "  'chocolate, lindt, lindor, smooth, white, filling, truffles, calories, carbs, 450, sodium,',\n",
       "  'peach, white, fresh, carbs, medium, calories, net, 2, 3, sodium, 100, mg, grape, values,',\n",
       "  'potato, chilis, skins, appetizer, calories, flesh, white, steam, 390, baked, carbs, sodium, food,',\n",
       "  'teeth, tooth, household, bleach, whiten, white, clorox, dentistry, dangerous, canal, sodium, regular, usage,',\n",
       "  'grilled, ham, generic, bread, cheese, spread, white, calories, carbs, sodium, qdoba,',\n",
       "  'cheese, lol, white, calorie, american, sodium, factslol, bread, americanamerican, 80, values, food,'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_indices = [127020,  140340,  367753,  405272,  706774,  865285,  962994,\n",
    "       1035936, 1095597, 1118707, 1250718, 1379532, 1794431, 1928216,\n",
    "       1993257, 2049300, 2098233, 2199433, 2199760, 2204684, 2234871,\n",
    "       2303640, 2327633, 2369164, 2459799, 2768104, 3112041]\n",
    "texts = text_dataset[text_indices][\"text\"]['input_ids'][:, :100]\n",
    "codes = text_codes[text_indices]\n",
    "codes[codes == -1] = 0\n",
    "\n",
    "t.batch_decode(texts, skip_special_tokens=True), t.batch_decode(codes, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-24 07:33:19,308] INFO (Dataset) initializing MSMARCO-doc memmap Query train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******train Query (28337):******\n",
      "what foods should not be eaten in kidney failure</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "******Text Code   (3088934):******\n",
      "<pad> kidney, foods, eat, creatinine, creatine, creatinine2013, blood, high, level, snack, mean,</s><pad>\n",
      "******Sorted Text Code for train:******\n",
      "['kidney, foods, eat, creatinine, creatine, level, high, blood, creatinine2013, snack, mean,', 'kidney, eat, foods, creatinine, creatine, blood, creatinine2013, high, level, mean, snack,', 'kidney, foods, eat, blood, creatinine, creatine, level, creatinine2013, high, mean, snack,']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check query_codes\n",
    "model = \"iter0\"\n",
    "\n",
    "query_sets = [\"train\"]\n",
    "dyn_text_codes = []\n",
    "qrels = []\n",
    "query_datasets = []\n",
    "for query_set in query_sets:\n",
    "    qrel = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    dyn_text_codes.append(\n",
    "        np.memmap(\n",
    "            f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/{model}/{query_set}/codes.mmp\",\n",
    "            mode=\"r\",\n",
    "            dtype=np.int32\n",
    "        ).reshape(len(qrel), -1, code_length).copy()\n",
    "    )\n",
    "    qrels.append(qrel)\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "# text_idx: [[(qrel_idx, query_idx)], ...,]\n",
    "tidx_2_qrel_query_idx_pair = defaultdict(lambda: [[] for _ in range(len(query_sets))])\n",
    "for query_set_idx, qrel in enumerate(qrels):\n",
    "    for j, rel in enumerate(qrel):\n",
    "        tidx_2_qrel_query_idx_pair[rel[1]][query_set_idx].append((j, rel[0]))\n",
    "# new_docs = {}\n",
    "# for k,v in tidx_2_qrel_query_idx_pair.items():\n",
    "#     has_common = 0\n",
    "#     for x in v:\n",
    "#         has_common += len(x) > 0\n",
    "#     if has_common == len(query_sets):\n",
    "#         new_docs[k] = v\n",
    "# tidx_2_qrel_query_idx_pair = new_docs\n",
    "same_count = 0\n",
    "same = False\n",
    "demo = 1\n",
    "\n",
    "for tidx, qindices in tidx_2_qrel_query_idx_pair.items():\n",
    "    text_code = text_codes[tidx]\n",
    "    text_code[text_code == -1] = 0\n",
    "    # print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "    if tidx != 3088934:\n",
    "        continue\n",
    "\n",
    "    for query_set_idx, query_set in enumerate(query_sets):\n",
    "        query_dataset = query_datasets[query_set_idx]\n",
    "        qindice = qindices[query_set_idx]\n",
    "\n",
    "        for qrel_idx, qidx in qindice:\n",
    "            dyn_text_code = dyn_text_codes[query_set_idx][qrel_idx]\n",
    "            dyn_text_code[dyn_text_code == -1] = 0\n",
    "\n",
    "            for c in dyn_text_code:\n",
    "                if (c == text_code).all():\n",
    "                    same_count += 1\n",
    "                    same = True\n",
    "                    # if demo:\n",
    "                    #     print(f'******{query_set} Query ({qidx}):******\\n{t.decode(query_dataset[qidx][\"query\"][\"input_ids\"])}')\n",
    "                    #     print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "                    #     print(f\"******Sorted Text Code for {query_set}:******\\n{t.decode(dyn_text_code)}\")\n",
    "                else:\n",
    "                    same = False\n",
    "            if not same and demo:\n",
    "                print(f'\\n******{query_set} Query ({qidx}):******\\n{t.decode(query_dataset[qidx][\"query\"][\"input_ids\"])}')\n",
    "                print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "                print(f\"******Sorted Text Code for {query_set}:******\\n{t.batch_decode(dyn_text_code, skip_special_tokens=True)}\")\n",
    "\n",
    "                x = input()\n",
    "                if x == \"s\":\n",
    "                    raise StopIteration\n",
    "\n",
    "same_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-24 06:42:11,421] INFO (Dataset) initializing MSMARCO-doc memmap Query dev dataset...\n",
      "[2024-01-24 06:42:12,315] INFO (Dataset) initializing MSMARCO-doc memmap Query train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************Model1 wins: (24, 2236571)************\n",
      "train Queries                                     : ['what does it mean is a government job i']\n",
      "Query                                             : government does do\n",
      "Target Anchor (term)                              : government, what, politics, answer, job, laws, reference, charge, branches, forms, enforces, carries, romans,\n",
      "False Positive Anchor                             : chile, government, chilean, type, president, answers, what, 44chile, govt, republic,\n",
      "************Model2 wins: (34, 2108113)************\n",
      "train Queries                                     : ['cost to send postcards', 'what is the price to mail a postcard']\n",
      "Query                                             : how much is the stamp to send a card\n",
      "Target Anchor (term)                              : postcard, card, postal, mail, cost, 49, send, post, much, 34, usps, stamp, inches, 6,\n",
      "False Positive Anchor                             : postcard, card, postcards2, postcards3, postcards5, postcards4, postcards10, postcards7, post,\n",
      "*************Model2 wins: (69, 26750)*************\n",
      "train Queries                                     : ['which olfactory nerve controls smell']\n",
      "Query                                             : the function of the olfactory nerve concerns\n",
      "Target Anchor (term)                              : nerve, olfactory, smell, regeneration, sense, taste, nose, disorders, odors, function, responsible, rose,\n",
      "False Positive Anchor                             : olfactory, nerve, smell, cn, odors, cranial, sensory, olfaction,\n",
      "************Model2 wins: (106, 12455)*************\n",
      "train Queries                                     : ['what manufacturer makes the road hugger tire']\n",
      "Query                                             : types of road hugger tires\n",
      "Target Anchor (term)                              : gt, radial, tires, hugger, 275, road, who, makes, rugger, anywhere, yahoo,\n",
      "False Positive Anchor                             : tire, radial, hugger, g, road, tread, performance, traction, treadwear, t, premium,\n",
      "***********Model2 wins: (171, 2088878)************\n",
      "train Queries                                     : [\"what was kennedy's flexible response\"]\n",
      "Query                                             : what about flexible response\n",
      "Target Anchor (term)                              : flexible, response, kennedy, respond, strategic, defense, eisenhower, retaliation,\n",
      "False Positive Anchor                             : eisenhower, eisenhowerreligion, eisenhowerchildren, president, biography,\n",
      "************Model2 wins: (367, 466317)************\n",
      "train Queries                                     : ['coo salaries']\n",
      "Query                                             : what does a coo do at a hospital\n",
      "Target Anchor (term)                              : hospital, hospital3what, hospital2pay, salary, operating, chief, coo, ihs, officer, ceo,\n",
      "False Positive Anchor                             : hospital, coo, duties, responsibilities, ceo, chief, medicare, executive, department, officer, administrative, organizations, roles,\n",
      "***********Model2 wins: (410, 1215369)************\n",
      "train Queries                                     : ['what is a hispanos']\n",
      "Query                                             : what does hispanic mean?\n",
      "Target Anchor (term)                              : hispanic, hispanico, hispanicization2, hispanicus, spanish, hispano, spain1,\n",
      "False Positive Anchor                             : hispanic, hispanicus, hispanico, latino, spanish, latinoamericano, mean,\n",
      "************Model2 wins: (447, 500183)************\n",
      "train Queries                                     : ['what safety means to me ideas']\n",
      "Query                                             : what does safety mean\n",
      "Target Anchor (term)                              : safety, safe, sauvete, comnoun, danger, browse, tee, catch, freedom, lock,\n",
      "False Positive Anchor                             : safety, safetysafe, safe, securityany, security, citesafety, safegiving, sauvete, danger, defensive,\n",
      "************Model1 wins: (470, 583327)************\n",
      "train Queries                                     : ['what is common occurrence for granite']\n",
      "Query                                             : what does the word granite\n",
      "Target Anchor (term)                              : granite, graniteland, granum, rock, consists, continents, occurring, binary, origin, plagioclase,\n",
      "False Positive Anchor                             : granite, granitenoun, ”, english, pronunciation, cambridge, definition, dictionary, translations, granito, rock,\n",
      "***********Model1 wins: (479, 1035480)************\n",
      "train Queries                                     : ['what increases prolactin']\n",
      "Query                                             : what drugs cause elevated prolactin level\n",
      "Target Anchor (term)                              : elevated, prolactin, levels, prolactinoma, blood, hyperprolactinemia, thyroid, raise,\n",
      "False Positive Anchor                             : prolactin, pregnancy, prolactina, prolactinoma, pituitary, hormone, milk,\n",
      "***********Model2 wins: (530, 2493147)************\n",
      "train Queries                                     : ['what is the function of cyclin proteins?']\n",
      "Query                                             : what is a cyclin\n",
      "Target Anchor (term)                              : cyclin, hunt, cdk, kinase, wikipedia, timothy, proteins, cell,\n",
      "False Positive Anchor                             : cytokinesis, cytokinesisnavigation, cytokinetic, division, dictionary, comprised, membrane,\n",
      "***********Model2 wins: (550, 3088934)************\n",
      "train Queries                                     : ['what foods should not be eaten in kidney failure']\n",
      "Query                                             : foods to raise my creatinine level\n",
      "Target Anchor (term)                              : kidney, foods, eat, creatinine, creatine, creatinine2013, blood, high, level, snack, mean,\n",
      "False Positive Anchor                             : kidney, creatinine, creatine, urine, blood, tested, waste, cystatin, gfr, meat, produced,\n"
     ]
    }
   ],
   "source": [
    "# cases that either res1 or res2 succeeds\n",
    "\n",
    "model1_result = load_pickle(f\"data/cache/MSMARCO-doc/retrieve/AutoTSG/dev/trie.pkl\")\n",
    "model2_result = load_pickle(f\"data/cache/MSMARCO-doc/retrieve/AutoTSG/dev/set.pkl\")\n",
    "\n",
    "code_type1 = \"term\"\n",
    "code_type2 = \"term\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 32\n",
    "\n",
    "text_codes1 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type1}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "text_codes2 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type2}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "positive_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    positive_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train\"]\n",
    "query_datasets = []\n",
    "queries = defaultdict(list)\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        queries[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "for qidx, positive in positives.items():\n",
    "    # if qidx != 3315:\n",
    "    #     continue\n",
    "    positive = positive[0]\n",
    "    model1_res = model1_result[qidx]\n",
    "    model2_res = model2_result[qidx]\n",
    "    query = query_dataset[qidx][\"query\"][\"input_ids\"]\n",
    "    showcase = False\n",
    "    if positive in model1_res and positive not in model2_res:\n",
    "    # if (positive in model1_res and model1_res.index(positive) < 5) and (positive not in model2_res or model2_res.index(positive) > 5):\n",
    "        text_code1 = text_codes1[positive]\n",
    "        text_code2 = text_codes2[positive]\n",
    "        false_pos_code = text_codes2[model2_res[0]]\n",
    "        string = f\"Model1 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    elif positive in model2_res and positive not in model1_res:\n",
    "        text_code1 = text_codes1[positive]\n",
    "        text_code2 = text_codes2[positive]\n",
    "        false_pos_code = text_codes1[model1_res[0]]\n",
    "        string = f\"Model2 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    if showcase:\n",
    "        has_train = False\n",
    "        all_q = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in queries[positive]:\n",
    "            new_query = t.decode(query_datasets[query_set_idx][query_idx][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "            all_q[query_set_idx].append(new_query)\n",
    "        for j, new_query in enumerate(all_q):\n",
    "            if len(new_query):\n",
    "                print(f\"{string:*^50}\")\n",
    "                print(f\"{query_sets[j] + ' Queries': <50}: {new_query}\")\n",
    "                has_train = True\n",
    "        if has_train:\n",
    "            # print(f\"{string:*^50}\")\n",
    "            print(f\"{'Query': <50}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "            print(f\"{f'Target Anchor ({code_type1})': <50}: {t.decode(text_code1[text_code1 != -1], skip_special_tokens=True)}\")\n",
    "            # print(f\"{f'Target Anchor ({code_type2})': <50}: {t.decode(text_code2[text_code2 != -1], skip_special_tokens=True)}\")\n",
    "            print(f\"{'False Positive Anchor': <50}: {t.decode(false_pos_code[false_pos_code != -1], skip_special_tokens=True)}\")\n",
    "            # print(f\"{f'Target Title ({code_type1})': <50}: {t.decode(text_dataset[positive]['text']['input_ids'][:20], skip_special_tokens=True)}\")\n",
    "            # print(f\"{f'Negative Title  ({code_type2})': <50}: {t.batch_decode(text_dataset[model2_res]['text']['input_ids'][:, :20], skip_special_tokens=True)}\")\n",
    "            x = input()\n",
    "            if x == \"s\":\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases for the failed dev queries and the corresponding train queries for a paticular model\n",
    "\n",
    "dataset = \"MSMARCO-passage\"\n",
    "retrieval_result = load_pickle(f\"data/cache/{dataset}/retrieve/AutoTSG/dev/retrieval_result.pkl\")\n",
    "positives = load_pickle(f\"data/cache/{dataset}/dataset/query/dev/positives.pkl\")\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "miss_queries = {}\n",
    "miss_docs = {}\n",
    "\n",
    "success_queries = {}\n",
    "success_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    res = retrieval_result[k]\n",
    "    if positive not in res:\n",
    "        miss_queries[k] = positive\n",
    "        miss_docs[positive] = k\n",
    "    else:\n",
    "        success_queries[k] = positive\n",
    "        success_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train\"]\n",
    "overlap_miss = defaultdict(list)\n",
    "overlap_success = defaultdict(list)\n",
    "\n",
    "query_datasets = []\n",
    "\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        if tidx in miss_docs:\n",
    "            overlap_miss[tidx].append((query_set_idx, qidx))\n",
    "        elif tidx in success_docs:\n",
    "            overlap_success[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "print(f\"Mean query number for missed queries: {mean_len(overlap_miss.values())}\\nMean query number for succeeded queries: {mean_len(overlap_success.values())}\")\n",
    "\n",
    "for k, v in miss_queries.items():\n",
    "    # positive = v[0]\n",
    "    positive = v\n",
    "\n",
    "    query = query_dataset[k][\"query\"][\"input_ids\"]\n",
    "    text = text_dataset[positive][\"text\"][\"input_ids\"]\n",
    "    text_code = text_codes[positive]\n",
    "    print(f\"\\n{f'Qidx: {k} Tidx: {positive}':*^40}\")\n",
    "    print(f\"{'Query': <20}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "    # print(f\"{'Target':*^25}\\n{t.decode(text, skip_special_tokens=True)}\")\n",
    "    print(f\"{'Anchor': <20}: {t.decode(text_code[text_code != -1], skip_special_tokens=True)}\")\n",
    "    print(f\"{'Text': <20}: {t.decode(text_dataset[positive]['text']['input_ids'][:100], skip_special_tokens=True)}\")\n",
    "    if positive in overlap_miss:\n",
    "        qindices = overlap_miss[positive]\n",
    "        queries = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in qindices:\n",
    "            queries[query_set_idx].append(t.decode(query_datasets[query_set_idx][query_idx]['query']['input_ids'], skip_special_tokens=True))\n",
    "        for j, query in enumerate(queries):\n",
    "            if len(query):\n",
    "                print(f\"{query_sets[j] + ' Queries': <20}: {query}\")\n",
    "    x = input()\n",
    "    if x == \"s\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting qid2idx: 32138350it [00:28, 1134842.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents missing in training set: 2893908\n"
     ]
    }
   ],
   "source": [
    "# create new query set based on an existing one\n",
    "\n",
    "# dataset = \"Top300k-filter\"\n",
    "# dataset = \"NQ320k\"\n",
    "dataset = \"MSMARCO-doc\"\n",
    "ori_query_set = \"doct5\"\n",
    "query_set = \"doct5-3\"\n",
    "k = 3\n",
    "\n",
    "try:\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{ori_query_set}/id2index.pkl\")\n",
    "except FileNotFoundError:\n",
    "    qid2idx = {}\n",
    "    with open(f\"{config.data_root}/{dataset}/queries.{ori_query_set}.tsv\") as f:\n",
    "        for qidx, line in enumerate(tqdm(f, desc=\"Collecting qid2idx\")):\n",
    "            qid = line.split(\"\\t\")[0]\n",
    "            qid2idx[qid] = qidx\n",
    "            \n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "qindices = []\n",
    "tid2qrels = defaultdict(list)\n",
    "\n",
    "train_positives = load_pickle(f\"data/cache/{dataset}/dataset/query/train/positives.pkl\")\n",
    "train_positives = set([x[0] for x in train_positives.values()])\n",
    "miss_docs = set(range(len(text_dataset))) - train_positives\n",
    "print(f\"number of documents missing in training set: {len(miss_docs)}\")\n",
    "\n",
    "with open(f\"{config.data_root}/{dataset}/qrels.{ori_query_set}.tsv\") as ori_qrel_file, open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, open(f\"{config.data_root}/{dataset}/queries.{ori_query_set}.tsv\") as ori_query_file, open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        qidx = qid2idx[qid]\n",
    "\n",
    "        # filter out the existing ones\n",
    "        # tidx = tid2idx[tid]\n",
    "        # if tidx in miss_docs:\n",
    "        #     tid2qrels[tid].append(line)\n",
    "        #     qindices.append(qidx)\n",
    "\n",
    "        # keep the first k elements\n",
    "        if len(tid2qrels[tid]) >= k:\n",
    "            continue\n",
    "        else:\n",
    "            tid2qrels[tid].append(line)\n",
    "            qindices.append(qidx)\n",
    "\n",
    "    qindices = set(qindices)\n",
    "    for i, line in enumerate(ori_query_file):\n",
    "        if i in qindices:\n",
    "            query_file.write(line)\n",
    "\n",
    "    for qrels in tid2qrels.values():\n",
    "        for line in qrels:\n",
    "            qrel_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false prune plot\n",
    "model = \"BOW_qg\"\n",
    "retrieval_result = load_pickle(f\"data/cache/{config.dataset}/retrieve/{model}/dev/bm25-trie.pkl\")\n",
    "positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "\n",
    "# code_type = \"words_comma_plus_stem\"\n",
    "# code_type = \"title_comma-first\"\n",
    "code_type = \"bm25_comma\"\n",
    "code_length = 34\n",
    "code_tokenizer = \"t5\"\n",
    "wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "wordset.fit(None)\n",
    "\n",
    "all_missed = 0\n",
    "for k, v in retrieval_result.items():\n",
    "    positive = positives[k][0]\n",
    "    if positive not in v:\n",
    "        all_missed += 1\n",
    "print(all_missed)\n",
    "\n",
    "cutoffs = [1,2,3,4,5]\n",
    "counts = [{\"all_missed\": 0, \"false_prune\": 0, \"wordset_match\": 0, \"decode_step\": i} for i in cutoffs]\n",
    "\n",
    "docs = wordset.docs\n",
    "stopwords = np.array([0, 1, -1])\n",
    "skip_qindices = {}\n",
    "\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    false_prune_count = 0\n",
    "    wordset_match_count = 0\n",
    "\n",
    "    for qidx, res in retrieval_result.items():\n",
    "        if qidx in skip_qindices:\n",
    "            continue\n",
    "\n",
    "        positive = positives[qidx][0]\n",
    "        if positive not in res:\n",
    "            gt_doc = docs[positive]\n",
    "            generated_doc = docs[res]\n",
    "            # invalid_at_cutoff = ((gt_doc[cutoff] == -1) * (generated_doc[:, cutoff] == -1)).astype(bool)\n",
    "            # print(invalid_at_cutoff)\n",
    "            \n",
    "            diff_at_cutoff = (gt_doc[cutoff] != generated_doc[:, cutoff])\n",
    "            if diff_at_cutoff.all():\n",
    "                false_prune_count += 1\n",
    "                skip_qindices[qidx] = 1\n",
    "                for gdoc in generated_doc:\n",
    "                    overlap = np.intersect1d(gt_doc, gdoc[:cutoff])  # n\n",
    "                    idx = (overlap[..., None] == stopwords[None, ...]).any(-1)\n",
    "                    overlap = overlap[~idx]                 # <=n\n",
    "                    if len(overlap):\n",
    "                        wordset_match_count += 1\n",
    "                        break\n",
    "\n",
    "        \n",
    "    if i == 0:\n",
    "        counts[i][\"all_missed\"] = all_missed\n",
    "    else:\n",
    "        all_missed = all_missed - counts[i - 1][\"false_prune\"]\n",
    "        counts[i][\"all_missed\"] = all_missed\n",
    "    \n",
    "    counts[i][\"false_prune\"] = false_prune_count\n",
    "    counts[i][\"wordset_match\"] = wordset_match_count\n",
    "    \n",
    "print(counts)\n",
    "data = pd.DataFrame(counts)\n",
    "data.drop(columns=[\"all_missed\"], inplace=True)\n",
    "data = data.melt(id_vars=[\"decode_step\"], value_vars=[\"false_prune\", \"wordset_match\"], value_name=\"count\")\n",
    "ax = sns.barplot(data, x=\"decode_step\", y=\"count\", hue=\"variable\")\n",
    "ax.set_ylabel(\"#Error Query\")\n",
    "ax.set_xlabel(\"Decode Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated codes and corresponding train queries\n",
    "\n",
    "dup_index = np.argwhere((text_codes == duplicates.loc[0].to_numpy()[:-1]).all(-1))[:, 0]\n",
    "\n",
    "# get train queries\n",
    "queries = [None for _ in range(len(dup_index))]\n",
    "arange = np.arange(len(queries))\n",
    "for x in train_dataset.qrels:\n",
    "    if x[-1] in dup_index:\n",
    "        idx = arange[dup_index == x[-1]][0]\n",
    "        queries[idx] = t.decode(train_dataset.query_datasets[0][x[-2]][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "print(len(dup_index))\n",
    "\n",
    "j = 0\n",
    "with open(\"/share/project/peitian/Data/Adon/Top300k/collection.tsv\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i in dup_index:\n",
    "            for k, v in positives.items():\n",
    "                if idx in v:\n",
    "                    print(f\"{'Dev Query':*^20}\\n{t.decode(query_dataset[k]['query']['input_ids'], skip_special_tokens=True)}\")\n",
    "            print(f\"{'Train Query':*^20}\\n{queries[j]}\")\n",
    "            print(line)\n",
    "            j += 1\n",
    "            x = input()\n",
    "            if x == \"s\":\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases that either res1 or res2 succeeds (different text_codes)\n",
    "\n",
    "model1_result = load_pickle(f\"data/cache/NQ/retrieve/BOW_doct5-miss-doc/dev/best.pkl\")\n",
    "model2_result = load_pickle(f\"data/cache/NQ/retrieve/BOW_doct5-miss-doc/dev/50.pkl\")\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "positives = load_pickle(\"data/cache/NQ/dataset/query/dev/positives.pkl\")\n",
    "positive_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    positive_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train-sub\", \"doct5-miss-sub\"]\n",
    "query_datasets = []\n",
    "queries = defaultdict(list)\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        queries[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "code_type1 = \"words_comma\"\n",
    "code_length1 = 34\n",
    "text_codes1 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type1}/{code_tokenizer}/{code_length1}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "code_type2 = \"words_comma\"\n",
    "code_length2 = 50\n",
    "text_codes2 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type2}/{code_tokenizer}/{code_length2}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "for qidx, positive in positives.items():\n",
    "    positive = positive[0]\n",
    "    model1_res = model1_result[qidx]\n",
    "    model2_res = model2_result[qidx]\n",
    "    query = query_dataset[qidx][\"query\"][\"input_ids\"]\n",
    "    showcase = False\n",
    "    if positive in model1_res and positive not in model2_res:\n",
    "        text_code = text_codes1[positive]\n",
    "        false_pos_code = text_codes2[model2_res[0]]\n",
    "        string = f\"Model1 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "    \n",
    "    elif positive in model2_res and positive not in model1_res:\n",
    "        text_code = text_codes2[positive]\n",
    "        false_pos_code = text_codes1[model1_res[0]]\n",
    "        string = f\"Model2 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    if showcase:\n",
    "        print(f\"{string:*^50}\")\n",
    "        print(f\"{'Query': <30}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "        print(f\"{'Target Anchor': <30}: {t.decode(text_code[text_code != -1], skip_special_tokens=True)}\")\n",
    "        string = f\"False Positive Anchor\"\n",
    "        print(f\"{string: <30}: {t.decode(false_pos_code[false_pos_code != -1], skip_special_tokens=True)}\")\n",
    "\n",
    "        all_q = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in queries[positive]:\n",
    "            query = t.decode(query_datasets[query_set_idx][query_idx][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "            all_q[query_set_idx].append(query)\n",
    "        for j, query in enumerate(all_q):\n",
    "            if len(query):\n",
    "                print(f\"{query_sets[j] + ' Queries': <30}: {query}\")        \n",
    "        x = input()\n",
    "        if x == \"s\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the document frequency of each code position\n",
    "code_type = \"bm25_comma-sample\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 26\n",
    "wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "wordset.fit(None)\n",
    "\n",
    "max_word_num = (wordset.docs != -1).sum(-1).max()\n",
    "doc_nums = np.zeros(max_word_num, dtype=np.int32)\n",
    "valid_nums = np.zeros(max_word_num, dtype=np.int32)\n",
    "\n",
    "for i in range(max_word_num):\n",
    "    i_th_position = wordset.docs[:, i]\n",
    "    valid_nums[i] = (i_th_position != -1).sum()\n",
    "    i_th_position = i_th_position[i_th_position != -1]\n",
    "    inverted_lists = wordset.inverted_lists[i_th_position]\n",
    "    doc_nums[i] = sum([len(x) for x in inverted_lists])\n",
    "wordset.inverse_vocab.shape, doc_nums / valid_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pseudo-queries from document\n",
    "dataset = \"MSMARCO-passage\"\n",
    "query_set = \"doc\"\n",
    "text_col = [2]\n",
    "query_length = 32\n",
    "\n",
    "with open(f\"{config.data_root}/{dataset}/collection.tsv\") as collection_file, open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file, open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file:\n",
    "    for tidx, line in enumerate(tqdm(collection_file)):\n",
    "        fields = line.split(\"\\t\")\n",
    "        tid = fields[0]\n",
    "        query_fields = [field.strip() for col_idx, field in enumerate(fields) if col_idx in text_col and len(field) > 1]\n",
    "        # maximum number of words\n",
    "        query = \" \".join(query_fields).split(\" \")[:query_length]\n",
    "        query = \" \".join(query)\n",
    "\n",
    "        query_file.write(\"\\t\".join([str(tidx), query]) + \"\\n\")\n",
    "        qrel_file.write(\"\\t\".join([str(tidx), \"0\", tid, \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter query sets with ANCE\n",
    "ori_query_set = \"doct5-5\"\n",
    "query_set = \"doct5-5-filter\"\n",
    "\n",
    "filter_model = \"ANCE\"\n",
    "filter_results = load_pickle(f\"data/cache/{config.dataset}/retrieve/{filter_model}/{ori_query_set}/retrieval_result.pkl\")\n",
    "filter_results = {k: v[:10] for k, v in filter_results.items()}\n",
    "\n",
    "tid2idx = load_pickle(f\"data/cache/{config.dataset}/dataset/text/id2index.pkl\")\n",
    "qid2idx = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{ori_query_set}/id2index.pkl\")\n",
    "\n",
    "with open(f\"{config.data_root}/{config.dataset}/qrels.{ori_query_set}.tsv\") as ori_qrel_file, open(f\"{config.data_root}/{config.dataset}/queries.{ori_query_set}.tsv\") as ori_query_file, open(f\"{config.data_root}/{config.dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, open(f\"{config.data_root}/{config.dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    qindices = set()\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        qidx = qid2idx[qid]\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_results[qidx]:\n",
    "            qrel_file.write(line)\n",
    "            qindices.add(qidx)\n",
    "    for i, line in enumerate(ori_query_file):\n",
    "        if i in qindices:\n",
    "            query_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter MSMARCO Top300k\n",
    "\n",
    "# filter_indices = []\n",
    "# for dup_idx in dup_indices:\n",
    "#     text = t.decode(text_dataset[dup_idx][\"text\"][\"input_ids\"], skip_special_tokens=True)\n",
    "#     text_length = len(text.split(\" \"))\n",
    "#     if text_length < 100:\n",
    "#         filter_indices.append(dup_idx)\n",
    "\n",
    "filter_indices = set(np.argwhere(df.duplicated().to_numpy())[:, 0].tolist())\n",
    "\n",
    "with open(\"/share/project/peitian/Data/Adon/Top300k/collection.tsv\") as ori_collection, open(\"/share/project/peitian/Data/Adon/Top300k/qrels.train.tsv\") as ori_train_qrels, open(\"/share/project/peitian/Data/Adon/Top300k/qrels.dev.tsv\") as ori_dev_qrels, open(\"/share/project/peitian/Data/Adon/Top300k-filter/collection.tsv\", \"w\") as collection, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.train.tsv\", \"w\") as train_qrels, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.dev.tsv\", \"w\") as dev_qrels:\n",
    "    # shutil.copy(\"/share/project/peitian/Data/Adon/Top300k/queries.train.tsv\", \"/share/project/peitian/Data/Adon/Top300k-filter/queries.train.tsv\")\n",
    "    # shutil.copy(\"/share/project/peitian/Data/Adon/Top300k/queries.dev.tsv\", \"/share/project/peitian/Data/Adon/Top300k-filter/queries.dev.tsv\")\n",
    "\n",
    "    for i, line in enumerate(ori_collection):\n",
    "        if i not in filter_indices:\n",
    "            collection.write(line)\n",
    "\n",
    "    tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k/dataset/text/id2index.pkl\")\n",
    "    for line in ori_train_qrels:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_indices:\n",
    "            continue\n",
    "        train_qrels.write(line)\n",
    "    \n",
    "    for line in ori_dev_qrels:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_indices:\n",
    "            continue\n",
    "        dev_qrels.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia DSI code to my format\n",
    "\n",
    "tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "code_type = \"DSI-semantic\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 10\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "count = 0\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/encoded_docid/t5_semantic_structured_top_300k.txt\") as f:\n",
    "    index = 0\n",
    "    for line in tqdm(f):\n",
    "        tid, code = line.strip().split()\n",
    "        tid = tid[1:-1].upper()\n",
    "        if tid in tid2idx:\n",
    "            count += 1\n",
    "            code = [int(x) for x in code.split(\",\")]\n",
    "            text_codes[index, 1:len(code)+1] = code\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia ultron code to my format\n",
    "\n",
    "dataset = \"Rand300k-filter\"\n",
    "tid2idx = load_pickle(f\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "code_type = \"ultron\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base={dataset}\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "# t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/{dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "count = 0\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/encoded_docid/t5_url_title_rand_300k.txt\") as f:\n",
    "    index = 0\n",
    "    for line in tqdm(f):\n",
    "        tid, code = line.strip().split()\n",
    "        tid = tid[1:-1].upper()\n",
    "        if tid in tid2idx:\n",
    "            count += 1\n",
    "            code = [int(x) for x in code.split(\",\")]\n",
    "            if len(code) > 33:\n",
    "                code = code[:33]\n",
    "                code[-1] = 1\n",
    "            text_codes[index, 1:len(code)+1] = code\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia doct5 to my format\n",
    "\n",
    "tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/msmarco-data/fake_query_10_all.txt\") as f, open(\"/share/project/peitian/Data/Adon/Top300k-filter/queries.doct5.tsv\", \"w\") as query_file, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.doct5.tsv\", \"w\") as qrel_file:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        tid, query = line.split(\"\\t\")\n",
    "        tid = tid[1:-1].upper()\n",
    "\n",
    "        qid = str(i)\n",
    "        if tid in tid2idx:\n",
    "            query_file.write(\"\\t\".join([qid, query]))\n",
    "            qrel_file.write(\"\\t\".join([qid, \"0\", tid, \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ultron code\n",
    "code_type = \"ultron\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "text_codes_rand = np.memmap(\n",
    "    makedirs(f\"data/cache/Rand300k-filter/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(-1, code_length).copy()\n",
    "text_codes_top = np.memmap(\n",
    "    makedirs(f\"data/cache/Top300k-filter/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(-1, code_length).copy()\n",
    "\n",
    "print(text_codes_rand.shape, text_codes_top.shape)\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=MS600k\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/MS600k/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "text_codes[:len(text_codes_rand)] = text_codes_rand\n",
    "text_codes[len(text_codes_rand):] = text_codes_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split code from MS600k\n",
    "\n",
    "# code_type = \"ANCE_hier\"\n",
    "code_type = \"NCI-bias\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 10\n",
    "# code_type_split = \"ANCE_hier_600k\"\n",
    "code_type_split = \"NCI_600k-bias\"\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=MS600k\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/MS600k/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32,\n",
    ").reshape(text_dataset.text_num, code_length).copy()\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=Rand300k-filter\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "rand_text_dataset = TextDataset(config)\n",
    "\n",
    "text_codes_rand = np.memmap(\n",
    "    makedirs(f\"data/cache/Rand300k-filter/codes/{code_type_split}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(rand_text_dataset.text_num, code_length),\n",
    ")\n",
    "text_codes_top = np.memmap(\n",
    "    makedirs(f\"data/cache/Top300k-filter/codes/{code_type_split}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(text_dataset.text_num - text_codes_rand.shape[0], code_length)    \n",
    ")\n",
    "\n",
    "text_codes_rand[:] = text_codes[:rand_text_dataset.text_num]\n",
    "text_codes_top[:] = text_codes[rand_text_dataset.text_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split some queries\n",
    "\n",
    "ori_dataset = \"NQ\"\n",
    "dataset = \"NQ-50k-seen\"\n",
    "query_set = \"doct5\"\n",
    "\n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "qids = set()\n",
    "\n",
    "with \\\n",
    "    open(f\"{config.data_root}/{ori_dataset}/qrels.{query_set}.tsv\") as ori_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, \\\n",
    "    open(f\"{config.data_root}/{ori_dataset}/queries.{query_set}.tsv\") as ori_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        if tid in tid2idx:\n",
    "            qids.add(qid)\n",
    "            qrel_file.write(line)\n",
    "    \n",
    "    for line in ori_query_file:\n",
    "        qid, query = line.split(\"\\t\")\n",
    "        if qid in qids:\n",
    "            query_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert t5 code to bert code\n",
    "\n",
    "code_type = \"chat\"\n",
    "# code_type = \"words-weight\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 50\n",
    "# code_length = 34\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "new_code_tokenizer = \"bert\"\n",
    "new_t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, new_code_tokenizer))\n",
    "\n",
    "new_text_codes = []\n",
    "max_length = 0\n",
    "for text_code in tqdm(text_codes):\n",
    "    text_code = text_code[text_code != -1]\n",
    "    decoded = t.decode(text_code, skip_special_tokens=True)\n",
    "    encoded = new_t.encode(decoded, padding=False)\n",
    "    if new_code_tokenizer == \"bert\":\n",
    "        encoded = new_t.encode(decoded, padding=False)[1:]\n",
    "    new_text_codes.append(encoded)\n",
    "    if len(encoded) > max_length:\n",
    "        max_length = len(encoded)\n",
    "\n",
    "# plus one for the leading padding token\n",
    "mmp_path = f\"data/cache/{config.dataset}/codes/{code_type}/{new_code_tokenizer}/{max_length + 1}/codes.mmp\"\n",
    "makedirs(mmp_path)\n",
    "new_codes_mmp = np.memmap(\n",
    "    mmp_path,\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), max_length + 1)\n",
    ")\n",
    "new_codes_mmp[:,1:] = -1\n",
    "\n",
    "for qrel_idx, code in enumerate(tqdm(new_text_codes)):\n",
    "    new_codes_mmp[qrel_idx, 1:len(code) + 1] = code\n",
    "\n",
    "print(f\"saving at {mmp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder words-comma in ascending df\n",
    "\n",
    "new_docs = np.zeros_like(wordset.docs) - 1\n",
    "for i, doc in enumerate(tqdm(wordset.docs)):\n",
    "    doc = doc[doc != -1]\n",
    "    inverted_lists = wordset.inverted_lists[doc]\n",
    "    dfs = [len(x) for x in inverted_lists]\n",
    "\n",
    "    new_doc = sorted(zip(doc, dfs), key=lambda x: x[1])\n",
    "    new_doc = [x[0] for x in new_doc]\n",
    "    new_docs[i, :len(new_doc)] = new_doc\n",
    "\n",
    "code_type = \"words-comma-df\"\n",
    "path = f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"\n",
    "makedirs(path)\n",
    "\n",
    "print(f\"saving at {path}...\")\n",
    "\n",
    "new_text_codes = np.memmap(\n",
    "    path,\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=text_codes.shape\n",
    ")\n",
    "new_text_codes[:, 0] = text_codes[:, 0]\n",
    "new_text_codes[:, 1:] = -1\n",
    "\n",
    "sep_token_id = int(text_codes[0][text_codes[0] != -1][-1])\n",
    "\n",
    "for i, new_doc in enumerate(tqdm(new_docs)):\n",
    "    new_doc = new_doc[new_doc != -1]\n",
    "    tokens = wordset.inverse_vocab[new_doc].reshape(-1)\n",
    "    tokens = tokens[tokens != -1].tolist()\n",
    "    tokens.append(sep_token_id)\n",
    "    new_text_codes[i, 1: len(tokens) + 1] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new documents to Top300k-filter\n",
    "tid2index = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as f, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\", \"w\") as g, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel:\n",
    "\n",
    "    tids = []\n",
    "    for i, line in enumerate(f):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid not in tid2index:\n",
    "            tids.append(tid)\n",
    "            g.write(line)\n",
    "    \n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid2index = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Rand300k-filter/dataset/text/id2index.pkl\")\n",
    "tindex2id = {v: k for k, v in tid2index.items()}\n",
    "dup_tids = {tindex2id[x] for x in dup_indices}\n",
    "\n",
    "# filtered 300k\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel:\n",
    "\n",
    "    tids = []\n",
    "    for i, line in enumerate(collection_file):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid in tid2index and tid not in dup_tids:\n",
    "            tids.append(tid)\n",
    "            new_collection_file.write(line)\n",
    "    \n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid2index = load_pickle(\"data/cache/Rand300k-filter/dataset/text/id2index.pkl\")\n",
    "tindex2id = {v: k for k, v in tid2index.items()}\n",
    "\n",
    "positives = load_pickle(\"data/cache/Rand300k-filter/dataset/query/dev/positives.pkl\")\n",
    "positives = set([v[0] for v in positives.values()])\n",
    "positive_tids = [tindex2id[x] for x in positives]\n",
    "complement = sample(list([k for k, v in tid2index.items() if v not in positives]), 100000 - len(positives))\n",
    "tids = set(positive_tids + complement)\n",
    "print(len(tids))\n",
    "\n",
    "# filtered 300k\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/queries.dev.backup.tsv\") as dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/queries.dev.tsv\", \"w\") as new_dev_query:\n",
    "\n",
    "    for i, line in enumerate(collection_file):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid in tids:\n",
    "            tids.add(tid)\n",
    "            new_collection_file.write(line)\n",
    "\n",
    "    qids = set()\n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)\n",
    "            qids.add(qid)\n",
    "\n",
    "    for line in dev_query:\n",
    "        qid = line.strip().split()[0]\n",
    "        if qid in qids:\n",
    "            new_dev_query.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\") as rand_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/collection.tsv\") as top_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\") as rand_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.dev.tsv\") as top_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/qrels.dev.tsv\", \"w\") as new_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/queries.dev.tsv\") as rand_dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/queries.dev.tsv\") as top_dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/queries.dev.tsv\", \"w\") as new_dev_query:\n",
    "\n",
    "    rand_tids = []\n",
    "    for line in rand_collection_file:\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        rand_tids.append(tid)\n",
    "        new_collection_file.write(line)\n",
    "    \n",
    "    top_tids = []\n",
    "    for line in top_collection_file:\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        top_tids.append(tid)\n",
    "        new_collection_file.write(line)\n",
    "\n",
    "    assert len(set(rand_tids).intersection(set(top_tids))) == 0\n",
    "\n",
    "    for line in rand_dev_query:\n",
    "        new_dev_query.write(line)\n",
    "    for line in top_dev_query:\n",
    "        new_dev_query.write(line)\n",
    "    for line in rand_dev_qrel:\n",
    "        new_dev_qrel.write(line)\n",
    "    for line in top_dev_qrel:\n",
    "        new_dev_qrel.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split NQ to Seen and Unseen for evaluating performance of adding new documents\n",
    "# d0_tindices, d0_train_positives, d0_dev_positives, d0_test_positives (3915)\n",
    "# d1_tindices, d1_test_positives (3915)\n",
    "\n",
    "train_positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/train/positives.pkl\")\n",
    "test_positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "\n",
    "tidx_2_train_query = defaultdict(list)\n",
    "for k, v in train_positives.items():\n",
    "    tidx_2_train_query[v[0]].append(k)\n",
    "tindices_with_train_query = set([x[0] for x in train_positives.values()])\n",
    "\n",
    "tidx_2_test_query = defaultdict(list)\n",
    "for k, v in test_positives.items():\n",
    "    tidx_2_test_query[v[0]].append(k)\n",
    "tindices_with_test_query = set([x[0] for x in test_positives.values()])\n",
    "\n",
    "tindices = set(range(len(text_dataset)))\n",
    "\n",
    "test_qindices = set(test_positives.keys())\n",
    "d1_test_positives = {}\n",
    "# unseen documents on the entire training set\n",
    "d1_tindices = tindices - tindices_with_train_query\n",
    "for tidx in d1_tindices:\n",
    "    qindices = tidx_2_test_query[tidx]\n",
    "    for qidx in qindices:\n",
    "        d1_test_positives[qidx] = [tidx]\n",
    "\n",
    "# sampled queries\n",
    "d1_test_candidates = test_qindices - set(d1_test_positives.keys())\n",
    "# only keep the queries with 1 relevant document\n",
    "d1_test_qindices = set()\n",
    "for qidx in d1_test_candidates:\n",
    "    if len(tidx_2_test_query[test_positives[qidx][0]]) == 1:\n",
    "        d1_test_qindices.add(qidx)\n",
    "d1_test_candidates = random.sample(list(d1_test_qindices), 3915 - len(d1_test_positives))\n",
    "for qidx in d1_test_candidates:\n",
    "    d1_test_positives[qidx] = test_positives[qidx]\n",
    "\n",
    "d1_tindices = set([x[0] for x in d1_test_positives.values()])\n",
    "# sampled documents relevant to some train queries\n",
    "d1_candidates = tindices_with_train_query - tindices_with_test_query\n",
    "d1_tindices.update(random.sample(list(d1_candidates), 50000 - len(d1_tindices)))\n",
    "\n",
    "d0_test_positives = {}\n",
    "d0_test_tindices = tindices_with_test_query - d1_tindices\n",
    "for tidx in d0_test_tindices:\n",
    "    qindices = tidx_2_test_query[tidx]\n",
    "    for qidx in qindices:\n",
    "        d0_test_positives[qidx] = [tidx]\n",
    "\n",
    "# sanity\n",
    "d0_tindices = tindices - d1_tindices\n",
    "for tidx in d0_tindices:\n",
    "    assert tidx in tidx_2_train_query\n",
    "assert len(d0_test_positives) == 3915\n",
    "assert d0_test_tindices - d0_tindices == set()\n",
    "\n",
    "d0_train_positives = {}\n",
    "d0_dev_positives = {}\n",
    "\n",
    "d0_qindices = set()\n",
    "d0_tindices_with_multiple_rel = set()\n",
    "for tidx in d0_tindices:\n",
    "    qindices = tidx_2_train_query[tidx]\n",
    "    d0_qindices.update(qindices)\n",
    "    if len(qindices) > 1:\n",
    "        d0_tindices_with_multiple_rel.add(tidx)\n",
    "\n",
    "d0_dev_tindices = random.sample(list(d0_tindices_with_multiple_rel), 3000)\n",
    "for tidx in d0_dev_tindices:\n",
    "    qindices = tidx_2_train_query[tidx]\n",
    "    assert len(qindices) > 1\n",
    "    dev_qidx = random.sample(qindices, 1)[0]\n",
    "    d0_dev_positives[dev_qidx] = [tidx]\n",
    "    d0_qindices.remove(dev_qidx)\n",
    "\n",
    "for qidx in d0_qindices:\n",
    "    d0_train_positives[qidx] = train_positives[qidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/text/id2index.pkl\").items()}\n",
    "# 50k\n",
    "d0_dataset = \"NQ-50k-seen\"\n",
    "# the remaining 50k\n",
    "d1_dataset = \"NQ-50k-unseen\"\n",
    "\n",
    "train_qidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/query/train/id2index.pkl\").items()}\n",
    "dev_qidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/id2index.pkl\").items()}\n",
    "\n",
    "os.makedirs(f\"{config.data_root}/{d0_dataset}\", exist_ok=True)\n",
    "os.makedirs(f\"{config.data_root}/{d1_dataset}\", exist_ok=True)\n",
    "\n",
    "# with open(f\"{config.data_root}/{config.dataset}/collection.tsv\") as collection_file, \\\n",
    "#     open(f\"{config.data_root}/{config.dataset}/queries.train.tsv\") as train_query_file, \\\n",
    "#     open(f\"{config.data_root}/{config.dataset}/queries.dev.tsv\") as dev_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/collection.tsv\", \"w\") as d0_collection_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.train.tsv\", \"w\") as d0_train_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.dev.tsv\", \"w\") as d0_dev_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.test.tsv\", \"w\") as d0_test_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.train.tsv\", \"w\") as d0_train_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.dev.tsv\", \"w\") as d0_dev_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.test.tsv\", \"w\") as d0_test_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/collection.tsv\", \"w\") as d1_collection_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/queries.dev.tsv\", \"w\") as d1_test_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/qrels.dev.tsv\", \"w\") as d1_test_qrel_file:\n",
    "\n",
    "#     for tidx, line in enumerate(tqdm(collection_file)):\n",
    "#         if tidx in d0_tindices:\n",
    "#             d0_collection_file.write(line)\n",
    "#         if tidx in d1_tindices:\n",
    "#             d1_collection_file.write(line)\n",
    "    \n",
    "#     for qidx, line in enumerate(tqdm(train_query_file)):\n",
    "#         if qidx in d0_train_positives:\n",
    "#             positive = d0_train_positives[qidx][0]\n",
    "#             d0_train_query_file.write(line)\n",
    "#             d0_train_qrel_file.write(\"\\t\".join([train_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "#         elif qidx in d0_dev_positives:\n",
    "#             positive = d0_dev_positives[qidx][0]\n",
    "#             d0_dev_query_file.write(line)\n",
    "#             d0_dev_qrel_file.write(\"\\t\".join([train_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "\n",
    "#     for qidx, line in enumerate(tqdm(dev_query_file)):\n",
    "#         if qidx in d0_test_positives:\n",
    "#             positive = d0_test_positives[qidx][0]\n",
    "#             d0_test_query_file.write(line)\n",
    "#             d0_test_qrel_file.write(\"\\t\".join([dev_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "#         if qidx in d1_test_positives:\n",
    "#             positive = d1_test_positives[qidx][0]\n",
    "#             d1_test_query_file.write(line)\n",
    "#             d1_test_qrel_file.write(\"\\t\".join([dev_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_types = [\"ANCE_hier\", \"words_comma\", \"words_comma_plus_stem\", \"title\"]\n",
    "code_types = [\"id\"]\n",
    "code_lengths = [8]\n",
    "code_tokenizer = \"t5\"\n",
    "\n",
    "for code_type, code_length in zip(code_types, code_lengths):\n",
    "    text_codes = np.memmap(\n",
    "        f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "        mode=\"r\",\n",
    "        dtype=np.int32\n",
    "    ).reshape(len(text_dataset), code_length).copy()\n",
    "\n",
    "    d0_text_codes = np.memmap(\n",
    "        makedirs(f\"data/cache/{d0_dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "        mode=\"w+\",\n",
    "        dtype=np.int32,\n",
    "        shape=(len(d0_tindices), code_length)\n",
    "    )\n",
    "    d1_text_codes = np.memmap(\n",
    "        makedirs(f\"data/cache/{d1_dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "        mode=\"w+\",\n",
    "        dtype=np.int32,\n",
    "        shape=(len(d1_tindices), code_length)\n",
    "    )\n",
    "    counts = [0 for _ in range(4)]\n",
    "    for tidx, text_code in enumerate(text_codes):\n",
    "        if tidx in d0_tindices:\n",
    "            d0_text_codes[counts[0]] = text_code\n",
    "            counts[0] += 1\n",
    "        if tidx in d1_tindices:\n",
    "            d1_text_codes[counts[1]] = text_code\n",
    "            counts[1] += 1\n",
    "    assert all([counts[i] == len(eval(f\"d{i}_tindices\")) for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case for generation likelihood\n",
    "model = AM.from_pretrained(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/NQ/ckpts/BOW_qg/first+random3+em\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = default_collate([query_dataset[3315][\"query\"]])\n",
    "\n",
    "text_code = [0]\n",
    "# for x in \"executive office of president\".split(\" \"):\n",
    "# for x in \"white house communication director\".split(\" \"):\n",
    "for x in \"cristeta comerford executive chef\".split(\" \"):\n",
    "# for x in \"executive chef white house\".split(\" \"):\n",
    "# for x in \"white house executive chef\".split(\" \"):\n",
    "    text_code += t.encode(x, add_special_tokens=False) + [6]\n",
    "text_code += [1]\n",
    "text_code = default_collate([text_code])\n",
    "\n",
    "# text_code = default_collate([text_codes[108361].astype(np.int64)])\n",
    "\n",
    "text_code[text_code == -1] = 0\n",
    "display(t.decode(text_code[0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.plm(**query, decoder_input_ids=text_code).logits\n",
    "    logits = torch.log_softmax(logits, dim=-1)\n",
    "    logits = logits.gather(dim=-1, index=text_code[:, 1:, None])[0,:,0]\n",
    "    # logits[4] = logits[4] - 4\n",
    "    cum = logits.cumsum(dim=-1).numpy().round(3)\n",
    "    tokens = t.convert_ids_to_tokens(text_code[0])[1:]\n",
    "list(zip(cum, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave two query sets to form a unified one with all documents having the same number of queries\n",
    "\n",
    "k = 5\n",
    "dataset = \"NQ\"\n",
    "query_set = \"pad5\"\n",
    "main_query_set = \"train\"\n",
    "sup_query_set = \"doct5\"\n",
    "\n",
    "main_qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{main_query_set}/id2index.pkl\")\n",
    "sup_qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{sup_query_set}/id2index.pkl\")\n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "with \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{main_query_set}.tsv\") as main_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{sup_query_set}.tsv\") as sup_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{main_query_set}.tsv\") as main_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{sup_query_set}.tsv\") as sup_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "\n",
    "    new_qid = 0\n",
    "    # map qidx to a new query id\n",
    "    qidx2newid = {}\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{main_query_set}/id2index.pkl\")\n",
    "    main_tid2qidx = defaultdict(list)\n",
    "    for i, line in enumerate(main_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        # keep the first k elements\n",
    "        if len(main_tid2qidx[tid]) < k:\n",
    "            qidx = qid2idx[qid]\n",
    "            main_tid2qidx[tid].append(qidx)\n",
    "            qidx2newid[qidx] = new_qid\n",
    "            new_qid += 1\n",
    "\n",
    "    # slice main queries\n",
    "    for qidx, line in enumerate(main_query_file):\n",
    "        if qidx in qidx2newid:\n",
    "            qid, query = line.split(\"\\t\")\n",
    "            qid = qidx2newid[qidx]\n",
    "            query_file.write(\"\\t\".join([str(qid), query]))\n",
    "\n",
    "    # store qrel using new_qid\n",
    "    for tid, qindices in main_tid2qidx.items():\n",
    "        for qidx in qindices:\n",
    "            qid = qidx2newid[qidx]\n",
    "            qrel_file.write(\"\\t\".join([str(qid), \"0\", tid, \"1\"]) + '\\n')\n",
    "\n",
    "    qidx2newid = {}\n",
    "    sup_tid2qidx = defaultdict(list)\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{sup_query_set}/id2index.pkl\")\n",
    "    for i, line in enumerate(sup_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        # keep the first k elements\n",
    "        # pad to k\n",
    "        if len(main_tid2qidx[tid]) + len(sup_tid2qidx[tid]) < k:\n",
    "            qidx = qid2idx[qid]\n",
    "            sup_tid2qidx[tid].append(qidx)\n",
    "            qidx2newid[qidx] = new_qid\n",
    "            new_qid += 1\n",
    "    \n",
    "    # slice sup queries\n",
    "    for qidx, line in enumerate(sup_query_file):\n",
    "        if qidx in qidx2newid:\n",
    "            qid, query = line.split(\"\\t\")\n",
    "            qid = qidx2newid[qidx]\n",
    "            query_file.write(\"\\t\".join([str(qid), query]))\n",
    "\n",
    "    # store qrel using new_qid\n",
    "    for tid, qindices in sup_tid2qidx.items():\n",
    "        for qidx in qindices:\n",
    "            qid = qidx2newid[qidx]\n",
    "            qrel_file.write(\"\\t\".join([str(qid), \"0\", tid, \"1\"]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "778a5a6b0df35a46498564cf16af2e5ec016022ef7dc9d5934de67fcb1f6bfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
